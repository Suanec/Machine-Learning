# 线性回归

决策函数：$f(x) = w_1x_1 + w_2 x_2 + ... + w_d x_d + b = w^T x + b$

损失函数：squared error  $ err(\hat{y}, y) = (\hat{y}, y)^2 $

**最小化损失函数（参数学习/假设搜索）**

$$   (w^*,b^*) = \mathop{\arg\min}_{(w,b)} \sum\limits_{i1}^{m}(f(x_i)-y_i)^2=arg min_{(w,b)} \sum\limits_{i1}^{m}(y_i-wx_i-b)^2 $$

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear1.png)

**基于梯度下降的参数学习/假设搜索**

梯度下降法：求解无约束优化问题最简单、最经典的方法之一

当目标函数为凸函数时，局部极小点对应着函数的全局最小点，此时梯度下降法可确保收敛到全局最优解



考虑无约束优化问题$min_x f(x)$, 其中f(x)为连续可微函数

若能构造一个序列$x^0, x^1, x^2,...满足f(x^{t+1}) < f(x^t), t=0,1,2,...$

则不断执行该过程即可收敛到局部极小点

根据泰勒展开式有$f(x+\Delta x) ~= f(x) + \Delta x^T \nabla f(x) $

于是，欲满足$f(x+\Delta x) < f(x)$，可选择 $ \Delta x = -\gamma\nabla f(x)$

其中步长$\gamma$ 是一个小常数。这就是梯度下降法



为了找到一个函数的局部极小值，于是让x朝着当前点对应梯度（或者是近似梯度）的反方向行走一定的距离进行迭代搜索

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear2.jpg)

批量梯度下降，随机梯度下降，小批量梯度下降

迭代终止：
定义一个合理的阈值，当两次迭代之间的差值小于该阈值时，迭代结束;
设置一个大概的迭代步数;
设置函数阈值；

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear3.png)

# 线性分类(逻辑回归)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear4.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear5.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear6.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear7.png)

# 其他问题(从线性到非线性、从二分类到多分类、类别不平衡)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear8.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear9.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear10.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear11.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear12.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear13.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear14.png)

## 类别不平衡问题

当不同类别的训练样例数目差别很大的时候，将对一些传统机器学习模型的训练造成困扰。
困扰何在？
假设我们现在的任务是判断交易欺诈。
历史交易中的欺诈记录却很少。
欺诈行为带来的损失又是巨大的。所以希望尽可能地将欺诈行为给判断出来。
假设我们的训练数据包含10,000个真实交易以及10个欺诈交易。那么用这个数据训练模型（找寻假设）的时候，模型将所有的交易行为判断为真实交易即能够达到超过99.9%的准确度。一个“好模型”?

如何解决？

假定正例样本很少，反例样本很多。

**欠采样**：
去除一些反例样本使得正、反数据接近。然后再进行学习。
**过采样（重采样）**：
增加一些正例样本使得正、反数据接近。然后再进行学习。
**代价敏感**：
增加将正例样本错判为负例样本的惩罚。或采用“阈值移动”的方法。

#过拟合

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear15.png)

应该尽可能地学出适用于所有潜在样本的“普遍规律”

可能把训练样本自身的一些特点当做了所有潜在样本都会具有的一般性质，从而导致泛化能力下降。这种想象在机器学习中称为“过拟合”(overfitting)

与“过拟合”相对的是“欠拟合”(underfitting)，这是指对训练样本的一般性质尚未学好

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear16.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear17.jpg)

**偏差-方差窘境(bias-variance dilemma)**

假定我们能够控制学习算法的拟合能力
当学习器的拟合能力不够强时，训练数据的扰动不足以使得学习器产生显著变化，此时偏差主导了泛化错误率
当学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能够被学习器学到，方差逐渐主导了泛化错误率
当学习器的拟合能力非常强时，训练数据发生的轻微扰动都会使得学习器发生显著变化。若训练数据自身的、非全局的特性被学习器学到了，则发生了过拟合。

欠拟合则通常是由学习能力低造成的，增加模型的复杂度可以解决（e.g., 增加多项式的最高幂）

有多种因素可能导致过拟合，其中最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了
**过拟合是机器学习面临的关键障碍，研究重点。各类算法在提出之后，都有人在研究避免其过拟合的措施。**

特征数非常多时，用线性函数也可能导致过拟合。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear18.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear19.png)

L1正则化的求解可以使用近端梯度下降（Proximal Gradient Descent，PGD）
P. L. Combettesand V. R. Wajs, “Signal recovery by proximal forwardbackwardsplitting,” MultiscaleModeling and Simulation, vol. 4, no. 4, pp. 1168–1200, 2006.

结构风险

表达了我们希望获得具有何种性质的模型（例如希望获得复杂度较小的模型）。这是一种引入领域知识和用户意图的途径。另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。

# 参数调节

本质就是模型选择：不同的参数对应不同的模型

如何构建候选模型（参数）集合：
将各参数变量值的可行区间（可从小到大），划分为一系列的小区
产生对应各参数变量值组合
计算各参数组合下的模型的性能
逐一比较择优，从而求得最佳参数组合。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/linear20.png)