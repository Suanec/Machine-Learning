# 最大间隔

当一个数据集中的带有类别{+1, -1}的样本能够被某个线性决策面给完美地分开来的话。则称这个数据集是线性可分的linearly separable

$y_i (w^T x_i + b) ≥ 0, i=1, ..., m$

当训练数据集是线性可分的，存在无穷个线性决策面能够将两类数据正确划分

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm1.png)

支持向量Support Vector
支撑着两个对应于最大间隔的超平面：

$w^Tx+b=1$

$w^Tx+b=-1$

# 支持向量机

##线性可分支持向量机

$min \frac{1}{2} ||w||^2$

$s.t.$    $y_i(w^Tx_i+b)≥1, i=1, ..., m$

有约束，不能用梯度下降法。

凸二次规划(convex quadratic programming)问题，能直接用现成的优化计算包求解

更高效的方法：转化成对偶问题，然后用SMO算法

线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题

##对偶问题

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm2.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm3.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm4.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm5.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm6.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm7.png)

SMO的基本思路是先固定$\alpha_i$ 之外的所有参数，然后求$\alpha_i$ 上的极值。由于存在约束$\sum_{i=1}^n \alpha_i y_i =0$ ，若固定$\alpha_i$ 之外的其他变量，则$\alpha_i$ 可由其他变量导出。于是，SMO每次选择两个变量$\alpha_i$ 和 $\alpha_j$ ，并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：

选取一对需更新的变量$\alpha_i$ 和 $\alpha_j$

固定$\alpha_i$ 和 $\alpha_j$ 以外的参数，求解上式获得更新后的$\alpha_i$ 和$\alpha_j$

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm8.png)

## 软间隔

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm9.png)

离群点(Outliers):偏离群体的个别样本点
有可能为噪声，有可能不是
一定程度地考虑离群点

基于软间隔的线性支持向量机

所有样本都必须划分正确，这称为“硬间隔”（hard margin)

允许某些样本不满足约束，这称为“软间隔”（soft margin)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm10.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm11.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm12.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm13.png)

## 核函数

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/kernel1.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/kernel2.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/kernel3.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/kernel4.png)

## 支持向量机小结

当数据

线性可分时：线性可分支持向量机

接近线性可分时：基于软间隔的线性支持向量机

线性不可分时：核函数（非线性支持向量机）

## 支持向量机与正则化

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm14.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm15.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm16.png)

##支持向量回归SVR

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm17.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm18.png)

## 单类支持向量

###异常检测与单类分类问题

异常检测outlier detection / anomaly detection
即发现一组数据点中和大多数数据不同的数据点。

1. 欺诈检测：比如信用诈骗，电信诈骗，信用卡盗刷等
2. 入侵检测：搞安全的都知道，黑客或者白帽子经常设法攻击一些公司或者个人的服务器或PC机。
3. 生态灾难预警：各种自然灾害，极端天气的预警
4. 公共健康：禽流感等传染类疾病的预警
5. 反垃圾：但凡现在一个app用户有了一定的基数，立马成为各种黑产的目标，各种垃圾广告，垃圾邮件满天飞，让app的运营者不胜其扰，用户体验变的很差。

**非监督异常检测方法**能通过寻找与其他数据最不匹配的实例来检测出未标记测试数据的异常。
**监督式异常检测方法**需要一个已经被标记“正常”与“异常”的数据集，并涉及到训练分类器（与许多其他的统计分类问题的关键区别是异常检测的内在不均衡性）。
**半监督式异常检测方法根据一个给定的正常训练数据集创建一个表示正常行为的模型，然后检测由学习模型生成的测试实例的可能性。[单分类问题]**

### SVDD算法

(Support Vector Data Description)

学习出一个**最小**的超球面（超球面是指3维以上的空间中的球面，对应的2维空间中就是曲线，3维空间中就是球面，3维以上的称为超球面），**把这堆数据尽可能全都包起来**，识别一个新的数据点时，如果这个数据点落在超球面内，就是这个类，否则不是。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/svm19.png)

ScholkopfB., Platt J., Shawe-Taylor J., SmolaA.J. , and Williamson R.C. 2001. Estimating the support of a high-dimensional distribution. Neural Computation, 13(7): 1443–1471

http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html





















