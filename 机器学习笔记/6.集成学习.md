集成学习 Ensemble learning

# 集成思想

训练性能很好的单一分类器可能是困难的
是否可以通过将一系列性能一般的分类器集合成更好性能的组合分类器

个体学习器1，个体学习器2，...，个体学习器T——结合模块——输出

- 集成中只包含同种类型的个体学习器，这样的集成是“同质”的（homogeneous）

​       同质集成中的个体学习器亦称“基学习器”（base learner），相应的学习算法称为“基学习算法”（base learning algorithm）。

- 集成中也可包含不同类型的个体学习器，这样的集成是“异质”的（heterogenous）

  个体学习器一般不称为基学习器，常称为“组件学习器”（component learner）或直接称为个体学习器。

研究如何**构建**并**结合**多个学习器来完成学习任务

集成学习的好处：

* 提高稳健性，降低单一学习器造成的风险 
* 增大假设空间



##结合策略

（加权）平均法，（加权）投票法，绝对多数投票法，相对多数投票法等

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble1.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble2.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble3.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble4.png)

##构建准则

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble5.png)

# 集成学习算法

三种流行策略：Stacking, Bagging, Boosting

## 三种流行策略--Stacking

当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。

Stacking [Wolpert, 1992; Breiman, 1996b]是学习法的典型代表，把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器（meta-learner)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble6.png)

只考虑了初级学习器的训练误差，没有考虑泛化误差，会偏向于训练误差小的初级学习器

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble7.png)

##三种流行策略–Bagging & Boosting

个体学习器是同质的，在如何构建个体学习器上做文章

###三种流行策略–Bagging

自助采样法（bootstrap sampling）

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble8.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble9.png)

“包外估计”（out-of-bag estimate）

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble10.png)

### 三种流行策略–Boosting

Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，==再根据基学习器的表现对训练样本分布进行调整== （**使得个体分类器有所不同**），==使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器== （**使得组合分类器变强**）；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble11.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble12.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble13.png)

e1是误差，当e1≤1/2时，α1≥0，α1与e1成反比，误差率>50%的学习器不要。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble14.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble15.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble16.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble17.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble18.png)

####集成学习算法AdaBoost

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble19.png)

过程3：

“重赋权法(re-weighting)”：

根据样本分布为每个训练样本重新赋予一个权重

“重采样法(re-sampling)”：

根据样本分布对训练集重新进行采样，再用重采样而得到的样本集对基学习器进行训练

可获得“重启动”机会以避免训练过程过早停止

过程4：

分类误差率是被$G_m(x)$ 误分类样本的权值之和

过程7：

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble20.png)

## Bagging vesusBoosting

**Bagging主要减小了方差variance**

- Bagging采用重复取样,每个个体所采用的训练样本都是从训练集中按等概率抽取的，因此Bagging的各子网能够很好的覆盖训练样本空间，从而有着良好的稳定性。

**Boosting主要减少了偏差bias** （过拟合风险大）

- Boosting是基于权重的弱学习器的结合，采用迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，越来越接近真实值。
- Bagging的训练集的选择是随机的，各轮训练集之间相互独立；Bagging的各个预测函数没有权重；Bagging的各个预测函数可以并行生成，对于神经网络这样极为耗时的学习方法，Bagging可通过并行训练节省大量时间开销。
- Boosting的训练集的选择是独立的，各轮训练集的选择与前面各轮的学习结果有关；
- Boosting的各个预测函数只能顺序生成。

# 集成树算法

##随机森林RF

Random Forest = Bagging + CART

其中Bagging可以减小方差，CART是fully-grown的，可以减小偏差

随机森林实现多样化的子学习器：

- Bagging策略中的数据随机采样思想将会使得各个决策树在训练集方面有多样化。

- RF中另一个增加多样化的策略是候选属性集也将经历一个随机采样形成新的候选属性（子）集。

  CART不用剪枝，完整生长，（Bagging已经可以避免过拟合）。先随机采样形成候选属性集，再根据基尼指数选择划分属性。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble21.png)

## 梯度提升树GBDT

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble22.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble23.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble24.png)

xgboost(https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/ensemble25.png)

Complete Guide to Parameter Tuning in XGBoost(with codes in Python)

https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/

1. The XGBoostAdvantage

2. Understanding XGBoostParameters

   General Parameters: Guide the overall functioning
   Booster Parameters: Guide the individual booster (tree/regression) at each step
   Learning Task Parameters: Guide the optimization performed

3. Tuning Parameters (with Example)

  ​



