

特征工程 Feature Engineering

# 特征工程简介

## 特征工程

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe1.png)

## 特征类型

- 标称特征Nominal/Categorical feature
- 二值特征Binary feature
- 序数特征Ordinal feature
- 数值特征Numeric feature

离散特征， 连续特征

## 独热编码One-hot encoding

| is_red | is_green | is_blue |
| ------ | :------: | ------: |
| 1      |    0     |       0 |
| 0      |    1     |       0 |
| 0      |    0     |       1 |

哑变量编码 dummy encoding

| is_red | is_green |
| ------ | :------: |
| 1      |    0     |
| 0      |    1     |
| 0      |    0     |

# 特征预处理

- 特征标准化 Standardization
- 特征归一化、正规化 Normalization
- 特征平滑，如log平滑

# 维度约简

## 维度灾难

- 特征个数越多，分析特征、训练模型所需的时间就越长。
- 特征个数越多，模型也会越复杂，其推广能力会下降。

在高维情形下容易出现数据样本稀疏、距离计算困难等问题，这些对于大部分的机器学习方法而言都是严重障碍。这种现象称为“维度灾难curse of dimensionality”

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe2.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe3.png)

缓解维度灾难的重要途径

特征抽取feature extraction
通过某种数学变换将原始高维属性空间转变为一个低维“子空间subspace”

特征选择feature selection
从给定的特征集合中选取出相关特征子集

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe4.png)

## 特征选择

定义：

特征选择( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS ) ，或属性选择( Attribute Selection ) ，是指从全部特征中选取一个特征子集，使构造出来的模型更好。

目的：

解决维度灾难问题
选取出真正相关的特征简化了模型，使研究人员易于理解数据产生的过程。

### 子集搜索和子集评价

首先从特征全集中产生出一个特征子集，然后用评价函数对该特征子集进行评价，评价的结果与停止准则进行比较，若评价结果比停止准则好就停止，否则就继续产生下一组特征子集，继续进行特征选择。选出来的特征子集一般还要验证其有效性

### 特征选择类型

#### 过滤式 filter

先对数据集进行特征选择，然后再训练学习器，特征选择过程和后续学习方法无关。
这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe5.png)

**单变量评价+ Threshold Filter/Select K Best**

- Information Gain (IG)  信息增益
- $\chi^2$ statistic (CHI)  卡方统计量
- Mutual Information (MI)  互信息，度量两个变量的相似性

特征子集的重要性是由子集中每个特征所对应的相关统计量分量之和来决定。

指定一个阈值τ，然后选择比τ大的相关统计量分量所对应的特征即可；

也可指定欲选取的特征个数k，然后选择相关统计量分量最大的k个特征。

For regression: **f_regression, mutual_info_regression**

For classisification: **chi2, f_classif, mutual_info_classif**

**SelectKBest** removes all but the k highest scoring features

**SelectPercenttile** removes all but a user-specified highest scoring percentage of features.

```python
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
iris = load_iris()
X, y = iris.data, iris.target
print(X.shape)  # (150,4)
X_new = SelectKBest(chi2, k=2).fit_transform(X,y)
print(X_new.shape)  # (150,2)
```

可以先过滤，再用更高级的方法进行特征选择

#### 包裹式 wrapper

包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。
换言之，包裹式特征选择的目的就是为给定学习器选择最有利于其性能，“量身订做”的特征子集。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe6.png)

**学习器评价+ 启发式搜索**

子集评价：分类器错误率(Classifier error rate )
使用特定的分类器，用给定的特征子集对样本集进行分类，用分类的精度来衡量特征子集的好坏。
子集搜索：假如有p个特征，那么就会有2p种特征组合，每种组合对应了一个模型。由于每种特征组合都需要训练一次模型，而训练模型的代价实际上是很大的，无法枚举所有的特征子集来选最优的特征子集。简单的想法是利用启发式搜索方法：forward search（前向搜索）和backward search（后向搜索）。

序列前向选择( SFS , Sequential Forward Selection )

算法描述：特征子集X从空集开始，每次选择一个特征x加入特征子集X，使得特征函数J( X)最优。

算法评价：贪心算法，容易陷入局部最优值。缺点是只能加入特征而不能去除特征。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe7.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe8.png)

#### 包裹式 vs 过滤式

一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终**学习器性能**来看，包裹式特征选择比过滤式特征选择**更好**，但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的**计算开销**通常比过滤式特征选择**大得多**。

#### 嵌入式特征选择

嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。最典型的例子就是带剪枝的决策树和带L1范数的LASSO算法等。

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe9.png)

## 特征抽取

###主成分分析PCA

非监督算法，不考虑预测任务

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe10.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe11.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe12.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe13.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe14.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe15.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe16.png)

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe17.png)

###线性判别分析LDA

思想：同类样本的投影点尽可能接近、异类样本的投影点尽可能远离

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe18.png)

同类样例的投影点尽可能接近---投影之后同类数据的方差尽可能小
异类样例的投影点尽可能远离---投影之后异类数据的中心点距离尽可能大

C类数据——> C-1维线性无关

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe19.png)

PCA和LDA都是线性变换

### 多维尺度变换/多维缩放 MDS

多维尺度变换/多维缩放multi-dimensional scaling (MDS)

希望在低维空间中，原始空间样本之间的距离得以保持。



假设m个样本在原始空间的距离矩阵为D $ \in R^{m×m} $

第$i$ 行$j$ 列的元素$dist_{ij}$ 为样本$x_i$ 到 $x_j$ 的距离

样本在$d'$ 维空间的表示$Z\in R^{d'×m}, d' \leq d$

任意两个样本在$d'$ 维空间中的欧式距离等于原始空间中的距离

### 流形学习

样本点从二维空间中的矩形区域采样后以S型嵌入到三维空间，形成了一个流形空间。这个流形空间在局部具有欧式空间的性质，能用欧式距离来进行距离计算。
流形学习：将其映射回低维空间中，揭示其本质

学习流形学习理论可从Isomap算法看起

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe20.png)

# 特征工程利器-深度学习

![image](https://github.com/LinglingGreat/Quote/blob/master/img/ML/fe21.png)

